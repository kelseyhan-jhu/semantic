{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from utils import listdir, image_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictedVoxelDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.list_category = sorted(os.listdir('predicted_EVC_conv5'))[14:16]\n",
    "        path = []\n",
    "        categories = []\n",
    "        for i, c in enumerate(self.list_category):\n",
    "            path.extend([os.path.join(self.root_dir, c, s) for s in os.listdir(os.path.join(self.root_dir, c))])\n",
    "            nSample = len([s for s in os.listdir(os.path.join(self.root_dir, c))])\n",
    "            for n in range(nSample):\n",
    "                categories.append(i)\n",
    "        self.path = path\n",
    "        self.categories = categories\n",
    "    def __len__(self):\n",
    "        return len(self.path)\n",
    "    def __getitem__(self, idx):\n",
    "        #category = self.list_category[idx]\n",
    "        voxels = np.load(self.path[idx])[0]\n",
    "        sample = {'voxel': voxels, 'category': self.categories[idx]}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    \n",
    "class VisualFeatureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        if self.root_dir.split('.')[-1] == 'npy':\n",
    "            temp = np.load(self.root_dir, allow_pickle=True)\n",
    "            features = temp.item()\n",
    "        else:\n",
    "            features = torch.load(self.root_dir)\n",
    "        \n",
    "        categories = sorted(set(i.split('/')[0] for i in features.keys()))\n",
    "        classes = {c: n for n, c in enumerate(categories)}\n",
    "        self.images = [img for img in features.keys()]\n",
    "        self.classes = {img: classes[img.split('/')[0]] for img in features.keys()}\n",
    "        self.features = np.stack([f for f in features.values()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        feature = self.features[idx, :]\n",
    "        sample = {'feature': feature, 'category': self.classes[img]}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, nVox, nClass):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(nVox, nClass)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class NonLinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, nVox, nClass):\n",
    "        super(NonLinearClassifier, self).__init__()\n",
    "        self.linear = torch.nn.Linear(nVox, nVox)\n",
    "        self.linear2 = torch.nn.Linear(nVox, nClass)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = VisualFeatureDataset(root_dir='features_conv5_all.pth.npy')\n",
    "\n",
    "batch_size = 64\n",
    "n_iters = 10000\n",
    "epochs = n_iters / (len(feature_dataset)/batch_size)\n",
    "nFeature = 12544\n",
    "nClass = len(set(feature_dataset.classes.values()))\n",
    "lr_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(feature_dataset, [len(feature_dataset)-5000, 5000])\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "linear_model = LinearClassifier(nFeature, nClass)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=lr_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Iteration: 200. Loss: 7.508553981781006. Accuracy: 0.0%\n",
      "Test: Iteration: 200. Loss: 7.172121047973633. Accuracy: 1.58%\n",
      "Train: Iteration: 400. Loss: 6.313730716705322. Accuracy: 9.375%\n",
      "Test: Iteration: 400. Loss: 6.561035633087158. Accuracy: 4.62%\n",
      "Train: Iteration: 600. Loss: 5.779475688934326. Accuracy: 18.75%\n",
      "Test: Iteration: 600. Loss: 6.101731777191162. Accuracy: 8.22%\n",
      "Train: Iteration: 800. Loss: 5.2257819175720215. Accuracy: 25.0%\n",
      "Test: Iteration: 800. Loss: 5.4199628829956055. Accuracy: 12.0%\n",
      "Train: Iteration: 1000. Loss: 4.238009452819824. Accuracy: 37.5%\n",
      "Test: Iteration: 1000. Loss: 5.790889739990234. Accuracy: 15.62%\n",
      "Train: Iteration: 1200. Loss: 3.8164777755737305. Accuracy: 40.625%\n",
      "Test: Iteration: 1200. Loss: 5.8679914474487305. Accuracy: 19.18%\n",
      "Train: Iteration: 1400. Loss: 3.187185525894165. Accuracy: 68.75%\n",
      "Test: Iteration: 1400. Loss: 4.508225440979004. Accuracy: 22.0%\n",
      "Train: Iteration: 1600. Loss: 3.18043851852417. Accuracy: 60.9375%\n",
      "Test: Iteration: 1600. Loss: 4.750498294830322. Accuracy: 25.04%\n",
      "Train: Iteration: 1800. Loss: 2.466233253479004. Accuracy: 71.875%\n",
      "Test: Iteration: 1800. Loss: 4.197312355041504. Accuracy: 27.08%\n",
      "Train: Iteration: 2000. Loss: 2.1804676055908203. Accuracy: 84.375%\n",
      "Test: Iteration: 2000. Loss: 5.291374206542969. Accuracy: 29.32%\n",
      "Train: Iteration: 2200. Loss: 2.1040685176849365. Accuracy: 82.8125%\n",
      "Test: Iteration: 2200. Loss: 3.472590208053589. Accuracy: 30.68%\n",
      "Train: Iteration: 2400. Loss: 1.7576738595962524. Accuracy: 82.8125%\n",
      "Test: Iteration: 2400. Loss: 4.279658317565918. Accuracy: 32.3%\n",
      "Train: Iteration: 2600. Loss: 1.2139089107513428. Accuracy: 93.75%\n",
      "Test: Iteration: 2600. Loss: 3.8215739727020264. Accuracy: 33.58%\n",
      "Train: Iteration: 2800. Loss: 1.3627986907958984. Accuracy: 90.625%\n",
      "Test: Iteration: 2800. Loss: 4.592267990112305. Accuracy: 34.54%\n",
      "Train: Iteration: 3000. Loss: 1.521071434020996. Accuracy: 85.9375%\n",
      "Test: Iteration: 3000. Loss: 3.5826802253723145. Accuracy: 35.16%\n",
      "Train: Iteration: 3200. Loss: 1.094393253326416. Accuracy: 93.75%\n",
      "Test: Iteration: 3200. Loss: 3.7615201473236084. Accuracy: 36.12%\n",
      "Train: Iteration: 3400. Loss: 1.4444794654846191. Accuracy: 90.625%\n",
      "Test: Iteration: 3400. Loss: 3.4906649589538574. Accuracy: 36.62%\n",
      "Train: Iteration: 3600. Loss: 0.9188123941421509. Accuracy: 98.4375%\n",
      "Test: Iteration: 3600. Loss: 3.3069403171539307. Accuracy: 37.34%\n",
      "Train: Iteration: 3800. Loss: 0.9300947785377502. Accuracy: 93.75%\n",
      "Test: Iteration: 3800. Loss: 4.544277191162109. Accuracy: 37.62%\n",
      "Train: Iteration: 4000. Loss: 0.8369413018226624. Accuracy: 93.75%\n",
      "Test: Iteration: 4000. Loss: 3.588484525680542. Accuracy: 37.98%\n",
      "Train: Iteration: 4200. Loss: 0.6943473815917969. Accuracy: 96.875%\n",
      "Test: Iteration: 4200. Loss: 3.7739365100860596. Accuracy: 38.58%\n",
      "Train: Iteration: 4400. Loss: 0.8304579854011536. Accuracy: 93.75%\n",
      "Test: Iteration: 4400. Loss: 3.0009613037109375. Accuracy: 38.66%\n",
      "Train: Iteration: 4600. Loss: 0.8761084079742432. Accuracy: 93.75%\n",
      "Test: Iteration: 4600. Loss: 3.8112261295318604. Accuracy: 39.24%\n",
      "Train: Iteration: 4800. Loss: 0.7772473096847534. Accuracy: 95.3125%\n",
      "Test: Iteration: 4800. Loss: 4.2307939529418945. Accuracy: 39.5%\n",
      "Train: Iteration: 5000. Loss: 0.570606529712677. Accuracy: 98.4375%\n",
      "Test: Iteration: 5000. Loss: 2.7737064361572266. Accuracy: 39.64%\n",
      "Train: Iteration: 5200. Loss: 0.5273346304893494. Accuracy: 100.0%\n",
      "Test: Iteration: 5200. Loss: 3.3644466400146484. Accuracy: 40.04%\n",
      "Train: Iteration: 5400. Loss: 0.47908899188041687. Accuracy: 98.4375%\n",
      "Test: Iteration: 5400. Loss: 2.505307912826538. Accuracy: 40.28%\n",
      "Train: Iteration: 5600. Loss: 0.5235479474067688. Accuracy: 96.875%\n",
      "Test: Iteration: 5600. Loss: 4.1043267250061035. Accuracy: 40.36%\n",
      "Train: Iteration: 5800. Loss: 0.3135374188423157. Accuracy: 100.0%\n",
      "Test: Iteration: 5800. Loss: 3.7736401557922363. Accuracy: 40.42%\n",
      "Train: Iteration: 6000. Loss: 0.27363333106040955. Accuracy: 100.0%\n",
      "Test: Iteration: 6000. Loss: 2.028762102127075. Accuracy: 40.7%\n",
      "Train: Iteration: 6200. Loss: 0.40506839752197266. Accuracy: 98.4375%\n",
      "Test: Iteration: 6200. Loss: 3.3014936447143555. Accuracy: 41.06%\n",
      "Train: Iteration: 6400. Loss: 0.32336097955703735. Accuracy: 100.0%\n",
      "Test: Iteration: 6400. Loss: 2.912977695465088. Accuracy: 41.0%\n",
      "Train: Iteration: 6600. Loss: 0.41002488136291504. Accuracy: 98.03921568627452%\n",
      "Test: Iteration: 6600. Loss: 2.9174857139587402. Accuracy: 41.4%\n",
      "Train: Iteration: 6800. Loss: 0.431792289018631. Accuracy: 100.0%\n",
      "Test: Iteration: 6800. Loss: 3.167778491973877. Accuracy: 41.34%\n",
      "Train: Iteration: 7000. Loss: 0.3232640027999878. Accuracy: 98.4375%\n",
      "Test: Iteration: 7000. Loss: 2.795290946960449. Accuracy: 41.5%\n",
      "Train: Iteration: 7200. Loss: 0.3000829517841339. Accuracy: 100.0%\n",
      "Test: Iteration: 7200. Loss: 3.4112296104431152. Accuracy: 41.66%\n",
      "Train: Iteration: 7400. Loss: 0.31165987253189087. Accuracy: 100.0%\n",
      "Test: Iteration: 7400. Loss: 3.012451648712158. Accuracy: 41.6%\n",
      "Train: Iteration: 7600. Loss: 0.25529783964157104. Accuracy: 100.0%\n",
      "Test: Iteration: 7600. Loss: 3.1758880615234375. Accuracy: 41.6%\n",
      "Train: Iteration: 7800. Loss: 0.25008922815322876. Accuracy: 100.0%\n",
      "Test: Iteration: 7800. Loss: 2.807192802429199. Accuracy: 41.6%\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "acc = []\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, sample_batched in enumerate(train_loader):\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        voxels = Variable(torch.Tensor(sample_batched['feature'].float()))\n",
    "        categories = Variable(sample_batched['category'])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = linear_model(voxels)\n",
    "        train_loss = criterion(outputs, categories)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # train loss?\n",
    "        \n",
    "        iteration+=1\n",
    "        if iteration%200==0:\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if len(predicted) == len(categories):\n",
    "                train_total += outputs.size(0)\n",
    "                train_correct += (predicted == categories).sum()\n",
    "                train_accuracy = 100 * train_correct.item() / train_total\n",
    "            print(\"Train: Iteration: {}. Loss: {}. Accuracy: {}%\".format(iteration, train_loss.item(), train_accuracy))\n",
    "                 \n",
    "            \n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            for samples in test_loader:\n",
    "                voxels = Variable(torch.Tensor(samples['feature'].float()))\n",
    "                categories = Variable(samples['category'])\n",
    "                outputs = linear_model(voxels)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_loss = criterion(outputs, categories)\n",
    "                test_total += outputs.size(0)\n",
    "                if len(predicted) == len(categories):\n",
    "                    test_correct += (predicted == categories).sum()\n",
    "                    test_accuracy = 100 * test_correct.item() / test_total\n",
    "#                     acc.append(accuracy)\n",
    "#                     if accuracy < np.mean(acc[-3:].float()):\n",
    "                        \n",
    "            print(\"Test: Iteration: {}. Loss: {}. Accuracy: {}%\".format(iteration, test_loss.item(), test_accuracy))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(dataloader):\n",
    "#     print(i_batch, sample_batched['voxel'].shape, sample_batched['category'])\n",
    "\n",
    "batch_size = 2\n",
    "n_iters = 80\n",
    "epochs = n_iters / (len(voxel_dataset)/batch_size)\n",
    "nVox = voxel_dataset[0]['voxel'].shape[0]\n",
    "nClass = len(set(voxel_dataset.categories))\n",
    "lr_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_dataset = PredictedVoxelDataset(root_dir='./predictedvggall/subj001/LOC_conv5')\n",
    "print(len(voxel_dataset))\n",
    "print(voxel_dataset[0]['voxel'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearClassifier(nVox, nClass)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(linear_model.parameters(), lr=lr_rate)\n",
    "#optimizer = torch.optim.Adam(linear_model.parameters(), lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i, sample_batched in enumerate(train_loader):\n",
    "# #     print(i, sample_batched['voxel'].size(), sample_batched['category'])\n",
    "# print(len(test_loader))\n",
    "# for sample in test_loader:\n",
    "#     print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the linear model\n",
    "\n",
    "iteration = 0\n",
    "acc = []\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, sample_batched in enumerate(train_loader):\n",
    "        voxels = Variable(torch.Tensor(sample_batched['voxel'].float()))\n",
    "        categories = Variable(sample_batched['category'])\n",
    "        optimizer.zero_grad()\n",
    "        outputs = linear_model(voxels)\n",
    "\n",
    "        loss = criterion(outputs, categories)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # train loss?\n",
    "        \n",
    "        iteration+=1\n",
    "        if iteration%2==0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for samples in test_loader:\n",
    "                voxels = Variable(torch.Tensor(samples['voxel'].float()))\n",
    "                outputs = linear_model(voxels)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += outputs.size(0)\n",
    "                if len(predicted) == len(categories):\n",
    "                    correct += (predicted == categories).sum()\n",
    "                    accuracy = 100 * correct.item() / total\n",
    "#                     acc.append(accuracy)\n",
    "#                     if accuracy < np.mean(acc[-3:].float()):\n",
    "                        \n",
    "                    print(\"Iteration: {}. Loss: {}. Accuracy: {}%\".format(iteration, loss.item(), accuracy))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
